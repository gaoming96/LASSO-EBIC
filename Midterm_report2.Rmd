---
title: "Midterm Report"
author: "Ming Gao"
date: "March 30, 2018"
output:
  html_document: 
    toc: true # table of content true
    toc_float: 
      collapsed: false
    toc_depth: 5  # upto three depths of headings (specified by #, ## and ###)
    number_sections: False  ## if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/Gao/Desktop/dissertation/R/me/0320midterm")
```

## Preprocessing
First, we get the dimension of the dataset.
```{r pre1}
XX <- read.csv("july2017.csv", header = T) 
X <- XX[,-1] #first columne is the name
rownames(X) <- XX[,1]
dim(X)
```

### Deleting observations
There are `r sum(is.na(X))/dim(X)[1]/dim(X)[2]` NA values in total.

Investigate any structure of missing observations.
```{r pre2}
library(mice)
pattern <- md.pattern(X)
pattern <- pattern[,-ncol(pattern)] #delete the last chr of names which is "", no use at all
pattern_last <- pattern[nrow(pattern),]
pattern_last_df <- data.frame(variable_name=names(pattern_last),num_missing=pattern_last)
DT::datatable(pattern_last_df,rownames = FALSE)

```

Omit the variables that consist of more than 2184 NAs and samples contain more than 50 NAs.
```{r pre3}
 names <- names(which(pattern_last>2184))
X.1 <- X[,!(colnames(X) %in% names)]
rs <- apply(X.1,1,function(y) sum(is.na(y)))
X.2 <- X.1[-which(rs>50),]
row_name <- XX[,1]
row_name <- row_name[-which(rs>50)]
```
There are `r sum(is.na(X.2))/dim(X.2)[1]/dim(X.2)[2]` NA values in total now.

### filling in & assorting variables
```{r pre4}
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
# fill in NA value & determine factor variables
# if X.2[,i] has less than 6 unique variables, then we regard it as a factor
lu <- apply(X.2,2,function(y) length(unique(y,na.rm = TRUE)))
for(i in which(lu<6 | lu==6)) {
  X.2[is.na(X.2[,i]),i] <-getmode(X.2[,i])
  X.2[,i] <- factor(X.2[,i])
  }

# fill in NA value & determine numerical variables
for(i in which(lu>6)){
  X.2[,i] <- as.numeric(X.2[,i])
  temp_median <- median(X.2[,i], na.rm = TRUE) 
  X.2[is.na(X.2[,i]),i] <- temp_median
}

#delete variables that is constant through all the samples
X.3=X.2[,!(colnames(X.2) %in% names(which(lu==1)))]
rownames(X.3) <- row_name
X.3 <- X.3[,c(146,1:145,147:270)]
DT::datatable(X.3)
```

## lasso for mi (heart attack)

Choose mi: Heart attack (self-reported history) as the output, other variables except 

- CVD: Cardiovasuclar disease
- db: Diabetes (self-reported history)
- ht: Hypertension (self-reported history)
- st: Stroke (self-reported history)

as the input.

```{r l1}
rm(list=ls())
load("./normalized.Rdata")
library(glmnet)
names.wanted <- c("mi","st","ht","db","CVD")
y=data[,"mi"]
x=data[,!(colnames(data) %in% names.wanted)]
#hotcode
xd <- model.matrix(~.,x)[,-1] #delete the first columne which is the intercept, since glmnet will introduce an interception by default
```
### Train & test
The distribution of output:
```{r l2}
set.seed(1)
idx<-sample(nrow(xd),1/10*nrow(xd))
trainx<-xd[-idx,]
testx<-xd[idx,]
trainy<-y[-idx]
testy<-y[idx]
rm(xd)
cat("distribution of training output:")
summary(trainy)
cat("distribution of test output:")
summary(testy)
```
One hot encoding of input. Take a look at the test input:
```{r l3}
DT::datatable(testx)
```


Dimension of training input:(`r dim(trainx)`); dimension of test input:(`r dim(testx)`); length of training output:(`r length(trainy)`); length of test output:(`r length(testy)`).

### coefficient path of lambda

$L=-\frac{1}{n}loglik+\lambda||\beta||_1$

```{r l4}
fit = glmnet(trainx, trainy, family="binomial", nlambda=50, alpha=1)
plot(fit, xvar="lambda", label=TRUE)
```


### best lambda through CV: dev

The y-axis is the average of 10 dev on the leave out data. 

$dev=ResDev/N$, where $ResDev=-2*loglik$.

```{r l5}
lbdl <- matrix(0,nrow=4,ncol=6)
lbdl <- data.frame(lbdl)
rownames(lbdl)=c("CV_deviance","CV_misclassificaiton_rate","BIC","EBIC"); colnames(lbdl) <- c("lambda","num_non_zero","CCR_train","HUM_train","CCR_test","HUM_test")

library(doParallel)
cl <- makeCluster(3)
registerDoParallel(cl)

cvfit = cv.glmnet(trainx, trainy, family = "binomial", type.measure = "deviance",alpha=1,nfolds=10,parallel=T,standardize = T)
stopImplicitCluster()
plot(cvfit)
```

The best lambda is `r cvfit$lambda.min`.

#### Coefs
```{r l6}
lbdl[1,1] <- cvfit$lambda.min
cosp <- coef(cvfit, s=c(cvfit$lambda.min))
co_l_cv_mce <- as.matrix(cosp)[(1:cosp@Dim[1]) %in% summary(cosp)[,"i"],]
lbdl[1,2] <- length(co_l_cv_mce)
co_l_cv_mce
```

There are `r length(co_l_cv_mce)` no-zero variables.

#### Acurracy
```{r l7}
library(mcca)
pvtrain1 <- predict(cvfit,newx=trainx,type="response",s=c(cvfit$lambda.min))
lbdl[1,3] <- ccr(trainy,data.frame(1-pvtrain1,pvtrain1),"prob",2)
lbdl[1,4] <- hum(trainy,data.frame(1-pvtrain1,pvtrain1),"prob",2)
cat("Correct classification rate of traing data:",lbdl[1,3])
cat("Area under curve of traing data:",lbdl[1,4])

pvtest1 <- predict(cvfit,newx=testx,type="response",s=c(cvfit$lambda.min))
lbdl[1,5] <- ccr(testy,data.frame(1-pvtest1,pvtest1),"prob",2)
lbdl[1,6] <- hum(testy,data.frame(1-pvtest1,pvtest1),"prob",2)
cat("Correct classification rate of test data:",lbdl[1,5])
cat("Area under curve of test data:",lbdl[1,6])
```

### best lambda through CV: ME
```{r ll5}
cl <- makeCluster(3)
registerDoParallel(cl)

cvfit = cv.glmnet(trainx, trainy, family = "binomial", type.measure = "class",alpha=1,nfolds=10,parallel=T,standardize = T)
stopImplicitCluster()
plot(cvfit)
```

The best lambda is `r cvfit$lambda.min`.

#### Coefs
```{r ll6}
lbdl[2,1] <- cvfit$lambda.min
cosp <- coef(cvfit, s=c(cvfit$lambda.min))
co_l_cv_mce <- as.matrix(cosp)[(1:cosp@Dim[1]) %in% summary(cosp)[,"i"],]
lbdl[2,2] <- length(co_l_cv_mce)
co_l_cv_mce
```

There are `r length(co_l_cv_mce)` no-zero variables.

#### Acurracy
```{r ll7}
library(mcca)
pvtrain1 <- predict(cvfit,newx=trainx,type="response",s=c(cvfit$lambda.min))
lbdl[2,3] <- ccr(trainy,data.frame(1-pvtrain1,pvtrain1),"prob",2)
lbdl[2,4] <- hum(trainy,data.frame(1-pvtrain1,pvtrain1),"prob",2)
cat("Correct classification rate of traing data:",lbdl[2,3])
cat("Area under curve of traing data:",lbdl[2,4])

pvtest1 <- predict(cvfit,newx=testx,type="response",s=c(cvfit$lambda.min))
lbdl[2,5] <- ccr(testy,data.frame(1-pvtest1,pvtest1),"prob",2)
lbdl[2,6] <- hum(testy,data.frame(1-pvtest1,pvtest1),"prob",2)
cat("Correct classification rate of test data:",lbdl[2,5])
cat("Area under curve of test data:",lbdl[2,6])
```

### Best lambda through BIC

$BIC=-2loglik+df*log(n)$

```{r }
library(grpreg)
fit<- grpreg(X=trainx, y=trainy, penalty="grLasso",family = "binomial",group=1:ncol(trainx))
Lambda <- fit$lambda
xlim <- range(Lambda)
plot(Lambda,select(fit,crit="BIC")$IC,xlim=xlim,pch=19,type="o",ylab="BIC",col="red")
lbdl[3,1] <- select(fit,criterion = "BIC")$lambda
abline(v=lbdl[3,1],lwd=3)
```

#### Coefs
The best lambda is `r lbdl[3,1]`.
```{r }
cosp <- select(fit,criterion = "BIC")$beta
cosp[which(cosp!=0)]
lbdl[3,2] <- length(cosp[which(cosp!=0)])
```

There are `r length(cosp[which(cosp!=0)])` non-zero vbs.

##### Acurracy
```{r }
library(mcca)
pre <- predict(fit, X=trainx, type = "response",lambda = lbdl[3,1])
pre <- data.matrix(pre)
lbdl[3,3] <- ccr(y=as.numeric(trainy),d=data.frame(1-pre,pre),method="prob",k=2)
lbdl[3,4] <- hum(y=as.numeric(trainy),d=data.frame(1-pre,pre),method="prob",k=2)

pre <- predict(fit, X=testx, type = "response",lambda = lbdl[3,1])
pre <- data.matrix(pre)
lbdl[3,5] <- ccr(y=as.numeric(testy),d=data.frame(1-pre,pre),method="prob",k=2)
lbdl[3,6] <- hum(y=as.numeric(testy),d=data.frame(1-pre,pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbdl[3,5])
cat("Area under curve of test data:",lbdl[3,6])
```
### Best lambda through EBIC

$EBIC=-2loglik+df*log(n)+df*2*0.5*log(P)$

```{r }
library(grpreg)
Lambda <- fit$lambda
xlim <- range(Lambda)
bic <- select(fit,crit="BIC",df.method="active")$IC
ebic <- bic+2*0.5*log(dim(trainx)[2])*predict(fit,type="nvars")
plot(Lambda,ebic,xlim=xlim,pch=19,type="o",ylab="EBIC",col="red")
lbdl[4,1] <- Lambda[which(ebic==min(ebic,na.rm=T))]
abline(v=lbdl[4,1],lwd=3)
```

#### Coefs
The best lambda is `r lbdl[4,1]`.
```{r }
cosp <- predict(fit,type="coefficients",lambda = lbdl[4,1])
cosp[which(cosp!=0)]
lbdl[4,2] <- length(cosp[which(cosp!=0)])
```

There are `r length(cosp[which(cosp!=0)])` non-zero vbs.

#### Acurracy
```{r }
library(mcca)
pre <- predict(fit, X=trainx, type = "response",lambda = lbdl[4,1])
pre <- data.matrix(pre)
lbdl[4,3] <- ccr(y=as.numeric(trainy),d=data.frame(1-pre,pre),method="prob",k=2)
lbdl[4,4] <- hum(y=as.numeric(trainy),d=data.frame(1-pre,pre),method="prob",k=2)

pre <- predict(fit, X=testx, type = "response",lambda = lbdl[4,1])
pre <- data.matrix(pre)
lbdl[4,5] <- ccr(y=as.numeric(testy),d=data.frame(1-pre,pre),method="prob",k=2)
lbdl[4,6] <- hum(y=as.numeric(testy),d=data.frame(1-pre,pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbdl[4,5])
cat("Area under curve of test data:",lbdl[4,6])
```

## group lasso for mi
###	train & test
```{r }
vb <- c("trainx","trainy","testx","testy","lbdl")
rm(list=ls()[!ls() %in% vb])

testy=as.numeric(testy)-1
trainy=as.numeric(trainy)-1
```
### Index vector
```{r grp2}
load("./index.Rdata")
index <- index[-1]
index
```
### Coef path

$L=-\frac{1}{n}loglik+\lambda\sum_j ||\beta_j||_2$

```{r grp3}
library(grpreg)
fit <- grpreg(X=trainx, y=trainy, group=index, penalty="grLasso",family = "binomial")
plot(fit,label=T,log.l=T)
```

### Best lambda through CV: deviance
```{r grp4}
lbd <- matrix(0,nrow=4,ncol=6)
lbd <- data.frame(lbd)
rownames(lbd)=c("CV_deviance","CV_misclassificaiton_rate","BIC","EBIC"); colnames(lbd) <- c("lambda","num_non_zero","CCR_train","HUM_train","CCR_test","HUM_test")
cvfit <- cv.grpreg(X=trainx, y=trainy, group=index, penalty="grLasso",family = "binomial",nfolds=10,seed=1,trace=F)
lbd[1,1]<- cvfit$lambda.min
plot(cvfit)
```

##### Coefs

The best lambda is `r cvfit$lambda.min`.
```{r grp6}
cosp <- coef(cvfit,lambda=cvfit$lambda.min)
cosp[which(cosp!=0)]
lbd[1,2] <- length(cosp[which(cosp!=0)])
```

There are `r length(cosp[which(cosp!=0)])` non-zero vbs.

##### Acurracy
```{r grpac1}
library(mcca)
pre <- predict(cvfit, X=trainx, type = "response",lambda = lbd[1,1])
pre <- data.matrix(pre)
lbd[1,3] <- ccr(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[1,4] <- hum(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)

pre <- predict(cvfit, X=testx, type = "response",lambda = lbd[1,1])
pre <- data.matrix(pre)
lbd[1,5] <- ccr(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[1,6] <- hum(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbd[1,5])
cat("Area under curve of test data:",lbd[1,6])
```

### Best lambda through CV: misclassification error
```{r grp7}

plot(cvfit,type="pred",vertical.line = F)
cvfit_sum <- summary(cvfit)
lbd[2,1] <- cvfit_sum$lambda[which(cvfit_sum$pe ==min(cvfit_sum$pe))]
abline(v=log(lbd[2,1]),lwd=3)
```

The best lambda is `r lbd[2,1]`.

#### Coefs
```{r grp8}
cosp <- coef(cvfit,lambda=lbd[2,1])
lbd[2,2] <- length(cosp[which(cosp!=0)])
cosp[which(cosp!=0)]
```

There are `r length(cosp[which(cosp!=0)])` non-zero vbs.

#### Acurracy
```{r grpac2}
library(mcca)
pre <- predict(cvfit, X=trainx, type = "response",lambda = lbd[2,1])
pre <- data.matrix(pre)
lbd[2,3] <- ccr(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[2,4] <- hum(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)

pre <- predict(cvfit, X=testx, type = "response",lambda = lbd[2,1])
pre <- data.matrix(pre)
lbd[2,5] <- ccr(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[2,6] <- hum(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbd[2,5])
cat("Area under curve of test data:",lbd[2,6])
```

### Best lambda through BIC
```{r grp5}
Lambda <- fit$lambda
xlim <- range(Lambda)
plot(Lambda,select(fit,crit="BIC",df.method="default")$IC,xlim=xlim,pch=19,type="o",ylab="BIC",col="red")
lbd[3,1] <- select(fit,criterion = "BIC")$lambda
abline(v=lbd[3,1],lwd=3)
```

#### Coefs

The best lambda is `r lbd[3,1]`.
```{r }
cosp <- select(fit,criterion = "BIC")$beta
cosp[which(cosp!=0)]
lbd[3,2] <- length(cosp[which(cosp!=0)])
```

There are `r length(cosp[which(cosp!=0)])` non-zero vbs.

#### Acurracy
```{r grpac3}
library(mcca)
pre <- predict(fit, X=trainx, type = "response",lambda = lbd[3,1])
pre <- data.matrix(pre)
lbd[3,3] <- ccr(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[3,4] <- hum(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)

pre <- predict(cvfit, X=testx, type = "response",lambda = lbd[3,1])
pre <- data.matrix(pre)
lbd[3,5] <- ccr(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[3,6] <- hum(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbd[3,5])
cat("Area under curve of test data:",lbd[3,6])
```

### Best lambda through EBIC

```{r }
library(grpreg)
Lambda <- fit$lambda
xlim <- range(Lambda)
bic <- select(fit,crit="BIC",df.method="active")$IC
ebic <- bic+2*0.5*log(dim(trainx)[2])*predict(fit,type="nvars")
plot(Lambda,ebic,xlim=xlim,pch=19,type="o",ylab="EBIC",col="red")
lbd[4,1] <- Lambda[which(ebic==min(ebic,na.rm=T))]
abline(v=lbd[4,1],lwd=3)
```

#### Coefs
The best lambda is `r lbd[4,1]`.
```{r }
cosp <- predict(fit,type="coefficients",lambda = lbd[4,1])
cosp[which(cosp!=0)]
lbd[4,2] <- length(cosp[which(cosp!=0)])
```

There are `r length(cosp[which(cosp!=0)])` non-zero vbs.

##### Acurracy
```{r }
library(mcca)
pre <- predict(fit, X=trainx, type = "response",lambda = lbd[4,1])
pre <- data.matrix(pre)
lbd[4,3] <- ccr(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[4,4] <- hum(y=trainy+1,d=data.frame(1-pre,pre),method="prob",k=2)

pre <- predict(fit, X=testx, type = "response",lambda = lbd[4,1])
pre <- data.matrix(pre)
lbd[4,5] <- ccr(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
lbd[4,6] <- hum(y=testy+1,d=data.frame(1-pre,pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbd[4,5])
cat("Area under curve of test data:",lbd[4,6])
```

## sparse group lasso for mi
###	train & test
```{r }
vb <- c("trainx","trainy","testx","testy","lbdl","lbd","index")
rm(list=ls()[!ls() %in% vb])
trainy <- factor(trainy)
testy <- factor(testy)
library(msgl)
```

### Coef path

$-loglik+\lambda[(1-\alpha)\sum_{j=1}^J r_j*||\beta_j||_2+\alpha||\beta||_1]$

Where $r_j=(2*\#\{\beta_j\})^\frac{1}{2}$
```{r}
l <- lambda(x=trainx, classes=trainy,d=200,lambda.min=0.001, intercept = TRUE,standardize = TRUE,grouping =index)
fit <- msgl::fit(x=trainx, classes=trainy, alpha = 0.5, lambda = l,grouping = index,
                 standardize = TRUE,d = 200, intercept = TRUE)
be <- fit$beta
df <- matrix(0,340,200)
for (i in 1:200){
  tmp <- be[[i]]
  df[,i] <- as.matrix(tmp)[1,]
}
cl <- rainbow(340)
plot(0,0,xlim = c(0,0.05),ylim = c(-2.5,2),type = "n",xlab="lambda",ylab="coef")
for (i in 2:340){
  lines(fit$lambda,df[i,],col = cl[i],type = 'l')
}
```

### Best lambda through CV: deviance
```{r}
lbds <- matrix(0,nrow=4,ncol=6)
lbds <- data.frame(lbds)
rownames(lbds)=c("CV_deviance","CV_misclassificaiton_rate","BIC","EBIC"); colnames(lbds) <- c("lambda","num_non_zero","CCR_train","HUM_train","CCR_test","HUM_test")
library(doParallel)
cl <- makeCluster(3)
registerDoParallel(cl)
fit.cv <- cv(x=trainx, classes=trainy, fold = 10, alpha = 0.5, lambda = l,
             use_parallel = T,grouping =index,standardize = TRUE, intercept = TRUE)
id <- which(Err(fit.cv, type = "loglike")==min(Err(fit.cv, type = "loglike")))[1]
lbds[1,1]<- l[id]

Lambda <- l
xlim <- range(Lambda)
plot(Lambda,Err(fit.cv, type = "loglike"),xlim=xlim,pch=19,type="o",ylab="CV_dev",col="red")
abline(v=lbds[1,1],lwd=3)
```

##### Coefs

The best lambda is `r lbds[1,1]`.
```{r }
as.matrix(coef(fit, id))[1,]
lbds[1,2] <- length(as.matrix(coef(fit, id))[1,])
```

There are `r lbds[1,2]` non-zero vbs.

##### Acurracy
```{r }
library(mcca)
res <- predict(fit, trainx)
pre<- res$response[[id]]
lbds[1,3] <- ccr(y=trainy,d=t(pre),method="prob",k=2)
lbds[1,4] <- hum(y=trainy,d=t(pre),method="prob",k=2)

res <- predict(fit, testx)
pre<- res$response[[id]]
lbds[1,5] <- ccr(y=testy,d=t(pre),method="prob",k=2)
lbds[1,6] <- hum(y=testy,d=t(pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbds[1,5])
cat("Area under curve of test data:",lbds[1,6])
```

### Best lambda through CV: ME
```{r}
id <- which(Err(fit.cv, type = "rate")==min(Err(fit.cv, type = "rate")))[1]
lbds[2,1]<- l[id]
plot(Lambda,Err(fit.cv, type = "rate"),xlim=xlim,pch=19,type="o",ylab="CV_dev",col="red")
abline(v=lbds[2,1],lwd=3)
```

##### Coefs

The best lambda is `r lbds[2,1]`.
```{r }
as.matrix(coef(fit, id))[1,]
lbds[2,2] <- length(as.matrix(coef(fit, id))[1,])
```

There are `r lbds[2,2]` non-zero vbs.

##### Acurracy
```{r }
library(mcca)
res <- predict(fit, trainx)
pre<- res$response[[id]]
lbds[2,3] <- ccr(y=trainy,d=t(pre),method="prob",k=2)
lbds[2,4] <- hum(y=trainy,d=t(pre),method="prob",k=2)

res <- predict(fit, testx)
pre<- res$response[[id]]
lbds[2,5] <- ccr(y=testy,d=t(pre),method="prob",k=2)
lbds[2,6] <- hum(y=testy,d=t(pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbds[2,5])
cat("Area under curve of test data:",lbds[2,6])
```

### Best lambda through BIC
```{r}
lik <- rep(0,200)
res <- predict(fit, trainx)
for (i in 1:200){
  pre<- res$response[[i]]
  d=t(pre)
  lik[i] <- sum(log(d[trainy==0,1]))+ sum(log(d[trainy==1,2]))
}

df <- rep(0,200)
for (i in 1:200){
  df[i] <- length(as.matrix(coef(fit, i))[1,])
}
bic <- -1*2*lik+df*log(dim(trainx)[1])
Lambda <- fit$lambda
xlim <- range(Lambda)
plot(Lambda,bic,xlim=xlim,pch=19,type="o",ylab="BIC",col="red")
id <- which(bic==min(bic))
lbds[3,1] <- Lambda[id]
abline(v=lbds[3,1],lwd=3)
```

##### Coefs

The best lambda is `r lbds[3,1]`.
```{r }
as.matrix(coef(fit, id))[1,]
lbds[3,2] <- length(as.matrix(coef(fit, id))[1,])
```

There are `r lbds[3,2]` non-zero vbs.

##### Acurracy
```{r }
library(mcca)
#res <- predict(fit, trainx)
pre<- res$response[[id]]
lbds[3,3] <- ccr(y=trainy,d=t(pre),method="prob",k=2)
lbds[3,4] <- hum(y=trainy,d=t(pre),method="prob",k=2)

res <- predict(fit, testx)
pre<- res$response[[id]]
lbds[3,5] <- ccr(y=testy,d=t(pre),method="prob",k=2)
lbds[3,6] <- hum(y=testy,d=t(pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbds[3,5])
cat("Area under curve of test data:",lbds[3,6])
```

### Best lambda through EBIC
```{r}
ebic <- bic+df*2*1*log(dim(trainx)[2])
plot(Lambda,ebic,xlim=xlim,pch=19,type="o",ylab="EBIC",col="red")
id <- which(ebic==min(ebic))
lbds[4,1] <- Lambda[id]
abline(v=lbds[4,1],lwd=3)
```

##### Coefs

The best lambda is `r lbds[4,1]`.
```{r }
as.matrix(coef(fit, id))[1,]
lbds[4,2] <- length(as.matrix(coef(fit, id))[1,])
```

There are `r lbds[4,2]` non-zero vbs.

##### Acurracy
```{r }
library(mcca)
res <- predict(fit, trainx)
pre<- res$response[[id]]
lbds[4,3] <- ccr(y=trainy,d=t(pre),method="prob",k=2)
lbds[4,4] <- hum(y=trainy,d=t(pre),method="prob",k=2)

res <- predict(fit, testx)
pre<- res$response[[id]]
lbds[4,5] <- ccr(y=testy,d=t(pre),method="prob",k=2)
lbds[4,6] <- hum(y=testy,d=t(pre),method="prob",k=2)
cat("Correct classification rate of test data:",lbds[4,5])
cat("Area under curve of test data:",lbds[4,6])
```

## Comparison
```{r}
df <- rbind(lbdl,lbd,lbds)
DT::datatable(df)
save(df, file="./df.Rdata")
```

